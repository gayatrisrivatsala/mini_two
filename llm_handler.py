# /llm_handler.py - OPTIMIZED VERSION

import httpx
from typing import List, Dict
import logging
import asyncio
import re

# Import settings and performance configurations from the central config file.
# The 'settings' object loads variables (like API keys) from your .env file.
from config import settings, perf_config
from schemas import Chunk

# Configure logger using the level specified in the config file
logger = logging.getLogger(__name__)
logging.basicConfig(level=settings.LOG_LEVEL.upper())


async def _call_mistral_api(payload: dict, semaphore: asyncio.Semaphore, max_retries: int = settings.API_RETRY_ATTEMPTS) -> dict:
    """
    Enhanced API call with retry logic and error handling, using centralized settings.
    """
    headers = {
        "Authorization": f"Bearer {settings.MISTRAL_API_KEY}",
        "Content-Type": "application/json"
    }

    async with semaphore:
        for attempt in range(max_retries):
            try:
                async with httpx.AsyncClient(timeout=perf_config.LLM_TIMEOUT) as client:
                    if attempt > 0:
                        await asyncio.sleep(settings.API_RETRY_DELAY * (2 ** attempt))

                    response = await client.post(
                        "https://api.mistral.ai/v1/chat/completions",
                        headers=headers,
                        json=payload
                    )
                    response.raise_for_status()
                    return response.json()

            except httpx.HTTPStatusError as e:
                logger.error(f"Mistral API error (attempt {attempt + 1}/{max_retries}): {e.response.status_code} - {e.response.text}")
                if e.response.status_code == 429:
                    await asyncio.sleep(5 * (attempt + 1))
                    continue
                elif attempt == max_retries - 1:
                    return {"error": f"LLM API returned status {e.response.status_code}"}
            except Exception as e:
                logger.error(f"Unexpected error calling Mistral API (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return {"error": "Could not get a response from the language model"}

        return {"error": "Maximum retries exceeded"}

def clean_llm_output(raw_text: str) -> str:
    """
    Cleans and refines text generated by an LLM to produce a single, plain text paragraph.
    This function removes markdown (bold, headings), bullet points, and excessive whitespace.
    """
    if not raw_text:
        return ""

    # Replace escaped newlines and tabs with spaces
    text = raw_text.replace('\\n', ' ').replace('\\t', ' ')
    # Replace all newline characters with spaces to enforce a single paragraph
    text = text.replace('\n', ' ')

    # Remove markdown-style bolding (e.g., **text**) and italics (*text*)
    text = re.sub(r'\*+(.*?)\*+', r'\1', text)

    # Remove any markdown heading markers
    text = re.sub(r'#+\s+', '', text)

    # Remove any stray list markers or prefixes
    text = re.sub(r'^\s*([*\-â€¢]|\w\.)\s+', '', text)
    
    # Collapse all consecutive whitespace characters into a single space
    cleaned_text = re.sub(r'\s+', ' ', text).strip()

    return cleaned_text

def analyze_question_type(question: str) -> Dict[str, any]:
    """Analyze the question to determine the best answering strategy and extract key terms."""
    # This function is kept for potential future use or for other parts of the system,
    # but the main answer generation prompt is now unified.
    question_lower = question.lower()
    analysis = {
        'type': 'general', 'expects_number': False, 'expects_list': False,
        'expects_boolean': False, 'key_terms': [], 'urgency': 'normal'
    }

    if any(word in question_lower for word in ['how much', 'cost', 'price', 'amount', 'fee', '$']):
        analysis.update({'type': 'financial', 'expects_number': True})
    elif any(word in question_lower for word in ['what are', 'list', 'types of', 'kinds of']):
        analysis.update({'type': 'list', 'expects_list': True})
    elif any(word in question_lower for word in ['is', 'does', 'will', 'can', 'covered']):
        analysis.update({'type': 'boolean', 'expects_boolean': True})
    elif any(word in question_lower for word in ['when', 'how long', 'duration']):
        analysis['type'] = 'temporal'
    elif any(word in question_lower for word in ['where', 'which', 'who']):
        analysis['type'] = 'specific'

    analysis['key_terms'] = re.findall(r'\b(?:premium|deductible|coverage|claim|benefit|policy|exclusion|liability|copay|coinsurance|maximum|minimum|limit)\b', question_lower)
    return analysis

async def generate_hypothetical_answer(question: str, semaphore: asyncio.Semaphore) -> str:
    """Generate a sophisticated hypothetical answer (HyDE) using the configured small model."""
    prompt = f"""As an insurance expert, provide a detailed hypothetical answer for the following question as if it were from an insurance policy document. Write the answer as a single, clean paragraph.

QUESTION: {question}

HYPOTHETICAL ANSWER (as a single paragraph):"""

    payload = {
        "model": settings.SMALL_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3, "max_tokens": 300
    }
    result = await _call_mistral_api(payload, semaphore)
    if "error" in result:
        return question
    return clean_llm_output(result["choices"][0]["message"]["content"])

async def ask_mistral(question: str, relevant_chunks: List[Chunk], semaphore: asyncio.Semaphore) -> str:
    """Generates a final answer by synthesizing context into a single paragraph."""
    if not relevant_chunks:
        return "This information could not be found in the provided document."

    context = "\n\n" + "="*50 + "\n\n".join([f"CONTEXT SECTION {i+1} (Page {chunk.page_number}):\n{chunk.text}" for i, chunk in enumerate(relevant_chunks)])

    # This new prompt strictly enforces a single-paragraph, plain-text output.
    prompt = f"""You are an expert insurance document analyst. Your task is to provide a concise and accurate answer based ONLY on the provided context sections.

**CRITICAL FORMATTING RULES:**
1.  **SINGLE PARAGRAPH ONLY:** Synthesize all information into one continuous paragraph.
2.  **NO LISTS OR BULLETS:** Do NOT use bullet points, numbered lists, or any list-style formatting.
3.  **NO MARKDOWN:** Do NOT use bolding (`**`), italics (`*`), or headings (`#`).
4.  **BASE YOUR ANSWER ON FACTS:** Your entire answer must be derived exclusively from the provided context. Do not add external knowledge.
5.  **STATE LIMITATIONS:** If the information is not in the context, clearly state that within the paragraph.
6.  **BE EXACT:** Preserve all numbers, percentages, and technical terms exactly as they appear in the text.

{context}

**QUESTION:** {question}

**FINAL ANSWER (as a single, clean paragraph):**"""

    model_to_use = settings.LARGE_MODEL if settings.USE_LARGE_MODEL_FOR_COMPLEX else settings.SMALL_MODEL
    payload = {
        "model": model_to_use,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.0, "max_tokens": 800
    }
    result = await _call_mistral_api(payload, semaphore)
    if "error" in result:
        return f"Error generating answer: {result['error']}"

    # The cleaning function will ensure the output is a single, clean paragraph.
    cleaned_answer = clean_llm_output(result["choices"][0]["message"]["content"])
    
    if len(cleaned_answer) < perf_config.MIN_ANSWER_LENGTH:
        return "This information could not be found in the provided document."
    return cleaned_answer

async def generate_multi_perspective_answer(question: str, relevant_chunks: List[Chunk], semaphore: asyncio.Semaphore) -> str:
    """Generates an answer and, if enabled, verifies it with a second LLM call."""
    primary_answer = await ask_mistral(question, relevant_chunks, semaphore)

    should_verify = settings.ENABLE_ANSWER_VERIFICATION and any(term in question.lower() for term in ['cost', 'amount', 'coverage', 'limit', 'exclusion'])
    if not should_verify:
        return primary_answer

    verification_prompt = f"""Review the proposed answer for factual accuracy based ONLY on the provided context. The answer must be a single, clean paragraph.

CONTEXT FOR VERIFICATION:
{chr(10).join([f"Section {i+1}: {chunk.text}" for i, chunk in enumerate(relevant_chunks[:3])])}

ORIGINAL QUESTION: {question}
PROPOSED ANSWER: {primary_answer}

Is the proposed answer factually correct and presented as a single paragraph based on the context? If you find any errors or formatting issues, provide a corrected version in a single paragraph. If the answer is perfect, respond with "VERIFIED: " followed by the original answer."""
    payload = {
        "model": settings.LARGE_MODEL,
        "messages": [{"role": "user", "content": verification_prompt}],
        "temperature": 0.0, "max_tokens": 600
    }

    try:
        verification_result = await _call_mistral_api(payload, semaphore)
        if "error" not in verification_result:
            verified_answer_raw = verification_result["choices"][0]["message"]["content"]
            if verified_answer_raw.strip().startswith("VERIFIED:"):
                return primary_answer
            else:
                corrected = clean_llm_output(verified_answer_raw)
                if len(corrected) > len(primary_answer) * 0.5:
                    logger.info("Answer was improved by verification step.")
                    return corrected
    except Exception as e:
        logger.warning(f"Verification step failed with exception: {e}")

    return primary_answer